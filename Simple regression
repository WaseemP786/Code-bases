"""
Simple Linear Regression Example
--------------------------------
Includes:
- Train/test split
- Proper preprocessing
- Model fitting
- Core regression metrics
- Residual plots
- Coefficient interpretation

Dependencies:
pip install numpy pandas scikit-learn matplotlib
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

from sklearn.model_selection import train_test_split
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.impute import SimpleImputer
from sklearn.linear_model import LinearRegression
from sklearn.metrics import (
    mean_squared_error,
    mean_absolute_error,
    r2_score
)

# --------------------------------------------------
# 1. CREATE SYNTHETIC DATA (Replace with your data)
# --------------------------------------------------
np.random.seed(42)
n = 2000

df = pd.DataFrame({
    "age": np.random.normal(40, 10, n),
    "income": np.random.normal(50000, 15000, n),
    "city": np.random.choice(["JHB", "CPT", "DBN"], n),
})

# Create target variable
df["target"] = (
    2.5 * df["age"]
    + 0.003 * df["income"]
    + (df["city"] == "CPT") * 20
    + np.random.normal(0, 15, n)
)

# --------------------------------------------------
# 2. DEFINE FEATURES AND TARGET
# --------------------------------------------------
X = df.drop(columns=["target"])
y = df["target"]

numeric_cols = X.select_dtypes(include=["number"]).columns.tolist()
categorical_cols = X.select_dtypes(exclude=["number"]).columns.tolist()

# --------------------------------------------------
# 3. TRAIN / TEST SPLIT
# --------------------------------------------------
X_train, X_test, y_train, y_test = train_test_split(
    X, y,
    test_size=0.2,
    random_state=42
)

# --------------------------------------------------
# 4. PREPROCESSING PIPELINE
# --------------------------------------------------
numeric_pipeline = Pipeline([
    ("imputer", SimpleImputer(strategy="median")),
    ("scaler", StandardScaler())
])

categorical_pipeline = Pipeline([
    ("imputer", SimpleImputer(strategy="most_frequent")),
    ("onehot", OneHotEncoder(handle_unknown="ignore"))
])

preprocessor = ColumnTransformer([
    ("num", numeric_pipeline, numeric_cols),
    ("cat", categorical_pipeline, categorical_cols)
])

# --------------------------------------------------
# 5. BUILD FULL MODEL PIPELINE
# --------------------------------------------------
model = Pipeline([
    ("preprocess", preprocessor),
    ("regressor", LinearRegression())
])

# Fit model
model.fit(X_train, y_train)

# --------------------------------------------------
# 6. PREDICTIONS
# --------------------------------------------------
y_pred = model.predict(X_test)

# --------------------------------------------------
# 7. EVALUATION METRICS
# --------------------------------------------------
rmse = np.sqrt(mean_squared_error(y_test, y_pred))
mae = mean_absolute_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

print("\n=== Regression Metrics ===")
print("RMSE:", round(rmse, 4))
print("MAE :", round(mae, 4))
print("R²  :", round(r2, 4))

# --------------------------------------------------
# 8. RESIDUAL DIAGNOSTICS
# --------------------------------------------------
residuals = y_test - y_pred

# Residual histogram
plt.hist(residuals, bins=30)
plt.title("Residual Distribution")
plt.xlabel("Residual")
plt.ylabel("Frequency")
plt.show()

# Residuals vs Predictions
plt.scatter(y_pred, residuals)
plt.axhline(0)
plt.title("Residuals vs Predictions")
plt.xlabel("Predicted Values")
plt.ylabel("Residuals")
plt.show()

# --------------------------------------------------
# 9. COEFFICIENT INTERPRETATION
# --------------------------------------------------
feature_names = model.named_steps["preprocess"].get_feature_names_out()
coefficients = model.named_steps["regressor"].coef_

coef_df = pd.DataFrame({
    "feature": feature_names,
    "coefficient": coefficients,
    "abs_coef": np.abs(coefficients)
}).sort_values("abs_coef", ascending=False)

print("\n=== Top Coefficients ===")
print(coef_df[["feature", "coefficient"]].head(10))

"""
Interpretation:
- Positive coefficient → increases prediction
- Negative coefficient → decreases prediction
- Larger magnitude → stronger impact
"""

"""
Simple Linear Regression (No Pipelines)

This is a junior-style implementation:
- Manual preprocessing
- Manual scaling
- Direct model fitting
- Basic evaluation metrics
- Simple residual diagnostics
- Coefficient output

Dependencies:
pip install numpy pandas scikit-learn matplotlib
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score

# --------------------------------------------------
# 1) CREATE SAMPLE DATA
# --------------------------------------------------
np.random.seed(42)
n = 1500

df = pd.DataFrame({
    "age": np.random.normal(40, 10, n),
    "income": np.random.normal(50000, 12000, n),
})

# Create a continuous target with noise
df["target"] = (
    2.5 * df["age"]
    + 0.003 * df["income"]
    + np.random.normal(0, 15, n)
)

# --------------------------------------------------
# 2) DEFINE FEATURES AND TARGET
# --------------------------------------------------
X = df[["age", "income"]]
y = df["target"]

# --------------------------------------------------
# 3) TRAIN / TEST SPLIT
# --------------------------------------------------
X_train, X_test, y_train, y_test = train_test_split(
    X, y,
    test_size=0.2,
    random_state=42
)

# --------------------------------------------------
# 4) BASIC DATA CLEANING (JUNIOR-STYLE)
# --------------------------------------------------
# If there were missing values, a junior might just fill them with the median.
# (This is simplistic, but common in early implementations.)
X_train = X_train.copy()
X_test = X_test.copy()

for col in X_train.columns:
    median_val = X_train[col].median()
    X_train[col] = X_train[col].fillna(median_val)
    X_test[col] = X_test[col].fillna(median_val)  # use train median to avoid leakage

# --------------------------------------------------
# 5) SCALE FEATURES (OPTIONAL FOR LINEAR REGRESSION, BUT COMMON)
# --------------------------------------------------
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)  # fit on train only
X_test_scaled = scaler.transform(X_test)

# --------------------------------------------------
# 6) FIT LINEAR REGRESSION MODEL
# --------------------------------------------------
model = LinearRegression()
model.fit(X_train_scaled, y_train)

# --------------------------------------------------
# 7) PREDICT
# --------------------------------------------------
y_pred = model.predict(X_test_scaled)

# --------------------------------------------------
# 8) EVALUATE
# --------------------------------------------------
rmse = np.sqrt(mean_squared_error(y_test, y_pred))
mae = mean_absolute_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

print("\n=== Regression Metrics ===")
print("RMSE:", round(rmse, 4))
print("MAE :", round(mae, 4))
print("RÂ²  :", round(r2, 4))

# --------------------------------------------------
# 9) SIMPLE RESIDUAL CHECKS
# --------------------------------------------------
residuals = y_test - y_pred

plt.figure()
plt.hist(residuals, bins=30)
plt.title("Residual Distribution")
plt.xlabel("Residual (y_true - y_pred)")
plt.ylabel("Count")
plt.show()

plt.figure()
plt.scatter(y_pred, residuals, s=12)
plt.axhline(0)
plt.title("Residuals vs Predictions")
plt.xlabel("Predicted")
plt.ylabel("Residual")
plt.show()

# --------------------------------------------------
# 10) COEFFICIENTS (INTERPRETATION)
# --------------------------------------------------
coef_df = pd.DataFrame({
    "feature": X.columns,
    "coefficient": model.coef_
})

print("\nModel Coefficients (on scaled features):")
print(coef_df)

print("\nIntercept:", model.intercept_)

"""
Notes:
- Because we scaled X, coefficients are in 'standard deviation units' (not original units).
- Positive coefficient means: as the feature increases, the prediction increases (holding others constant).
"""

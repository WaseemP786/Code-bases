import numpy as np\nimport pandas as pd\nfrom sklearn.datasets import make_classification\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, precision_recall_curve\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.calibration import calibration_curve\n\n# Generate a synthetic imbalanced dataset\nX, y = make_classification(n_classes=2, class_sep=2, weights=[0.9, 0.1], n_informative=3, n_redundant=1, flip_y=0, n_features=20, n_clusters_per_class=1, n_samples=1000, random_state=10)\n\n# Split the dataset\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n\n# Train a Random Forest classifier\nmodel = RandomForestClassifier()\nmodel.fit(X_train, y_train)\n\n# Make predictions and evaluate the model\nprobs = model.predict_proba(X_test)[:, 1]\nthresholds = np.arange(0.0, 1.0, 0.01)\n\n# Initialize lists to store metrics\nroc_auc = []\npr_auc = []\n\nfor threshold in thresholds:\n    predictions = (probs >= threshold).astype(int)\n    roc_auc.append(roc_auc_score(y_test, predictions))\n    precision, recall, _ = precision_recall_curve(y_test, probs)\n    pr_auc.append(np.trapz(recall, precision))\n\n# Find optimal threshold based on ROC AUC\noptimal_idx = np.argmax(roc_auc)\noptimal_threshold = thresholds[optimal_idx]\n\n# Final predictions using optimal threshold\npredictions = (probs >= optimal_threshold).astype(int)\n\n# Generate evaluation metrics\nprint(classification_report(y_test, predictions))\ncm = confusion_matrix(y_test, predictions)\nprint('Confusion Matrix:\n', cm)\n\n# Plotting ROC AUC\nplt.figure(figsize=(12, 6))\nplt.plot(thresholds, roc_auc, label='ROC AUC')\nplt.axvline(optimal_threshold, color='red', linestyle='--')\nplt.title('ROC AUC vs Threshold')\nplt.xlabel('Threshold')\nplt.ylabel('ROC AUC')\nplt.legend()\nplt.show()\n\n# Plot Precision-Recall AUC\nplt.figure(figsize=(12, 6))\nplt.plot(thresholds, pr_auc, label='PR AUC')\nplt.axvline(optimal_threshold, color='red', linestyle='--')\nplt.title('Precision-Recall AUC vs Threshold')\nplt.xlabel('Threshold')\nplt.ylabel('PR AUC')\nplt.legend()\nplt.show()\n\n# Calibration Curve\nplt.figure(figsize=(12, 6))\nprob_true, prob_pred = calibration_curve(y_test, probs, n_bins=10)\nplt.plot(prob_pred, prob_true, marker='o', label='Calibration Curve')\nplt.plot([0, 1], [0, 1], linestyle='--', label='Perfectly Calibrated')\nplt.title('Calibration Curve')\nplt.xlabel('Mean Predicted Probability')\nplt.ylabel('Fraction of Positives')\nplt.legend()\nplt.show()  \n
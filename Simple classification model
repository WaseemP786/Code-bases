"""
Simple Logistic Regression Classification Example
-------------------------------------------------
Binary classification with:
- Proper preprocessing
- Stratified split
- Core evaluation metrics
- ROC + PR curves
- Confusion matrix
- Coefficient interpretation

Dependencies:
pip install numpy pandas scikit-learn matplotlib
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

from sklearn.model_selection import train_test_split
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.impute import SimpleImputer
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import (
    accuracy_score,
    precision_score,
    recall_score,
    f1_score,
    roc_auc_score,
    average_precision_score,
    confusion_matrix,
    ConfusionMatrixDisplay,
    roc_curve,
    precision_recall_curve,
    classification_report
)

# --------------------------------------------------
# 1. CREATE SYNTHETIC DATA (Replace with your data)
# --------------------------------------------------
np.random.seed(42)
n = 2000

df = pd.DataFrame({
    "age": np.random.normal(40, 10, n),
    "income": np.random.normal(50000, 15000, n),
    "city": np.random.choice(["JHB", "CPT", "DBN"], n),
})

# Create binary target using logistic function
logit = (
    0.05 * df["age"]
    + 0.00005 * df["income"]
    + (df["city"] == "CPT") * 0.8
    - 4
)

prob = 1 / (1 + np.exp(-logit))
df["target"] = np.random.binomial(1, prob)

print("Positive rate:", df["target"].mean().round(3))

# --------------------------------------------------
# 2. DEFINE FEATURES AND TARGET
# --------------------------------------------------
X = df.drop(columns=["target"])
y = df["target"]

# Identify numeric and categorical columns
numeric_cols = X.select_dtypes(include=["number"]).columns.tolist()
categorical_cols = X.select_dtypes(exclude=["number"]).columns.tolist()

# --------------------------------------------------
# 3. TRAIN / TEST SPLIT (STRATIFIED)
# --------------------------------------------------
X_train, X_test, y_train, y_test = train_test_split(
    X, y,
    test_size=0.2,
    stratify=y,
    random_state=42
)

# --------------------------------------------------
# 4. PREPROCESSING PIPELINE
# --------------------------------------------------
numeric_pipeline = Pipeline([
    ("imputer", SimpleImputer(strategy="median")),
    ("scaler", StandardScaler())
])

categorical_pipeline = Pipeline([
    ("imputer", SimpleImputer(strategy="most_frequent")),
    ("onehot", OneHotEncoder(handle_unknown="ignore"))
])

preprocessor = ColumnTransformer([
    ("num", numeric_pipeline, numeric_cols),
    ("cat", categorical_pipeline, categorical_cols)
])

# --------------------------------------------------
# 5. BUILD FULL MODEL PIPELINE
# --------------------------------------------------
model = Pipeline([
    ("preprocess", preprocessor),
    ("classifier", LogisticRegression(max_iter=1000))
])

# Fit model
model.fit(X_train, y_train)

# --------------------------------------------------
# 6. PREDICTIONS
# --------------------------------------------------
y_pred = model.predict(X_test)
y_prob = model.predict_proba(X_test)[:, 1]  # Probability of positive class

# --------------------------------------------------
# 7. EVALUATION METRICS
# --------------------------------------------------
print("\n=== Classification Metrics ===")
print("Accuracy :", round(accuracy_score(y_test, y_pred), 4))
print("Precision:", round(precision_score(y_test, y_pred), 4))
print("Recall   :", round(recall_score(y_test, y_pred), 4))
print("F1 Score :", round(f1_score(y_test, y_pred), 4))
print("ROC AUC  :", round(roc_auc_score(y_test, y_prob), 4))
print("PR AUC   :", round(average_precision_score(y_test, y_prob), 4))

print("\n=== Classification Report ===")
print(classification_report(y_test, y_pred))

# --------------------------------------------------
# 8. CONFUSION MATRIX
# --------------------------------------------------
cm = confusion_matrix(y_test, y_pred)
disp = ConfusionMatrixDisplay(confusion_matrix=cm)
disp.plot()
plt.title("Confusion Matrix")
plt.show()

# --------------------------------------------------
# 9. ROC CURVE
# --------------------------------------------------
fpr, tpr, _ = roc_curve(y_test, y_prob)
plt.plot(fpr, tpr)
plt.plot([0, 1], [0, 1], linestyle="--")
plt.title("ROC Curve")
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.show()

# --------------------------------------------------
# 10. PRECISION-RECALL CURVE
# --------------------------------------------------
precision, recall, _ = precision_recall_curve(y_test, y_prob)
plt.plot(recall, precision)
plt.title("Precision-Recall Curve")
plt.xlabel("Recall")
plt.ylabel("Precision")
plt.show()

# --------------------------------------------------
# 11. INTERPRETING COEFFICIENTS
# --------------------------------------------------
# Get feature names after preprocessing
feature_names = model.named_steps["preprocess"].get_feature_names_out()

# Extract logistic regression coefficients
coefficients = model.named_steps["classifier"].coef_[0]

coef_df = pd.DataFrame({
    "feature": feature_names,
    "coefficient": coefficients,
    "abs_coef": np.abs(coefficients)
}).sort_values("abs_coef", ascending=False)

print("\n=== Top Coefficients (by magnitude) ===")
print(coef_df[["feature", "coefficient"]].head(10))

"""
Interpretation:
- Positive coefficient -> increases probability of class 1
- Negative coefficient -> decreases probability of class 1
- Larger magnitude -> stronger influence
"""
